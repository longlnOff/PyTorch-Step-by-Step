{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spoilers\n",
    "\n",
    "- build a function to perform training steps.\n",
    "- implement our own dataset class.\n",
    "- use data loaders to generate mini-batches.\n",
    "- build a function to perform mini-batch gradient descent.\n",
    "- evaluate the model on the validation set.\n",
    "- intergrate TensorBoard to monitor model training and evaluation.\n",
    "- save/checkpoint our model to disk.\n",
    "- load our model from disk to resume training or to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 15:32:15.341024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-06 15:32:16.228938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rethinking the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_generation/simple_linear_regression.py'\n",
    "%run -i '../data_preparation/v0.py'\n",
    "%run -i '../model_configuration/v0.py'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use higheer-order function to build training step\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the training loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    # Returns the function that will be called inside the training loop\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_configuration/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_configuration/v1.py'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set learning parameters\n",
    "learning_rate = 1e-3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create model and send to device\n",
    "model = nn.Sequential(nn.Linear(1,1)).to(device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Creates the training_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_configuration/v1.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v1.py'\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_training/v1.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.2197]], device='cuda:0')), ('0.bias', tensor([1.4200], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a dataset is represented by a regular Python class that inherits from the Dataset class. A dataset class must implement three methods:\n",
    "- __init__(self) : The constructor, it is called when we instantiate the dataset object. There is no need to load the whole dataset in the constructor. If your dataset is big, loading it at once would not be memory efficient. It is recommended to load the data lazily in the ___get_item___ method.\n",
    "\n",
    "- __get_item__(self, index): it allows the dataset to be indexed so that it can work like a list (dataset[i]).\n",
    "- __len__(self): it returns the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.6398]), tensor([2.2890]))\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.6398]), tensor([2.2890]))\n"
     ]
    }
   ],
   "source": [
    "#if a dataset is nothing more than a couple of tensors, we can use PyTorch's TensorDataset class\n",
    "train_datatensor = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_datatensor[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we need build a dataset anyway? -> Because we want to use a ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tell DataLoader which dataset to use, the desired mini-batch size and if we'd like to shuffle it or not, and that all. PyTorch's DataLoader will automatically create mini-batches for our."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8150],\n",
       "         [0.9011],\n",
       "         [0.4282],\n",
       "         [0.3418],\n",
       "         [0.5195],\n",
       "         [0.8504],\n",
       "         [0.5994],\n",
       "         [0.8295],\n",
       "         [0.3350],\n",
       "         [0.7141],\n",
       "         [0.8421],\n",
       "         [0.2950],\n",
       "         [0.0872],\n",
       "         [0.0349],\n",
       "         [0.0795],\n",
       "         [0.8969]]),\n",
       " tensor([[2.5023],\n",
       "         [1.0689],\n",
       "         [2.9750],\n",
       "         [1.5896],\n",
       "         [2.1105],\n",
       "         [2.0180],\n",
       "         [2.0363],\n",
       "         [2.6549],\n",
       "         [2.2896],\n",
       "         [1.8805],\n",
       "         [1.9373],\n",
       "         [2.8198],\n",
       "         [3.0079],\n",
       "         [2.4757],\n",
       "         [2.9086],\n",
       "         [2.7999]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0], next(iter(train_loader))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.6398],\n",
       "         [0.9011],\n",
       "         [0.0349],\n",
       "         [0.4886],\n",
       "         [0.5162],\n",
       "         [0.3418],\n",
       "         [0.2417],\n",
       "         [0.1033],\n",
       "         [0.4842],\n",
       "         [0.9766],\n",
       "         [0.8504],\n",
       "         [0.5994],\n",
       "         [0.6324],\n",
       "         [0.3668],\n",
       "         [0.0795],\n",
       "         [0.3338]]),\n",
       " tensor([[2.2890],\n",
       "         [2.8792],\n",
       "         [1.1649],\n",
       "         [2.0180],\n",
       "         [2.0511],\n",
       "         [1.7058],\n",
       "         [1.4962],\n",
       "         [1.3058],\n",
       "         [2.0114],\n",
       "         [3.0231],\n",
       "         [2.7999],\n",
       "         [2.2896],\n",
       "         [2.3468],\n",
       "         [1.7644],\n",
       "         [1.2484],\n",
       "         [1.7144]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../data_preparation/v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../data_preparation/v1.py'\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Builds Dataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# Builds DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v1.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_configuration/v1.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v2.py'\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        mini_batch_loss = train_step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "    \n",
    "    # Computes average loss over all mini-batches\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_training/v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[1.5415]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.3110], device='cuda:0'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v1.py'\n",
    "%run -i '../model_configuration/v1.py'\n",
    "%run -i '../model_training/v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[1.5415]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.3110], device='cuda:0'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch Inner Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(device, data_loader, step):\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        mini_batch_loss = step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "    \n",
    "    loss = np.mean(mini_batch_losses)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v1.py'\n",
    "%run -i '../model_configuration/v1.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v3.py'\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    loss = mini_batch(device=device,\n",
    "                      data_loader=train_loader,\n",
    "                      step=train_step)\n",
    "\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_training/v3.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[1.5415]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.3110], device='cuda:0'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../data_preparation/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../data_preparation/v2.py'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.as_tensor(x_train, dtype=torch.float32)\n",
    "y_tensor = torch.as_tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Perform split\n",
    "ration = 0.8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ration)\n",
    "n_val = n_total - n_train\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Builds a loader for each split\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "                        dataset=train_data, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v2.py'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step(model, loss_fn):\n",
    "    # Builds function that performs a step in the validation loop\n",
    "    def perform_val_step(x, y):\n",
    "        # Set model to eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output\n",
    "        # forwad pass\n",
    "        yhat = model(x)\n",
    "\n",
    "        # Step 2 - Computes the validation loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the validation loop\n",
    "    return perform_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_configuration/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_configuration/v2.py'\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1,1)).to(device=device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Create train step\n",
    "tran_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# Create validation step\n",
    "val_step = make_val_step(model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_configuration/v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v4.py'\n",
    "\n",
    "n_epochs = 200\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop - training\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # VALIDATION - no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step)\n",
    "        val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_training/v4.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v2.py'\n",
    "%run -i '../model_configuration/v2.py'\n",
    "%run -i '../model_training/v4.py'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrtier = SummaryWriter('../runs/test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x, dummy_y = next(iter(train_loader))\n",
    "wrtier.add_graph(model, dummy_x.to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrtier.add_scalars(\n",
    "    main_tag = 'loss',\n",
    "    tag_scalar_dict={'training': loss,\n",
    "                     'validation': val_loss},\n",
    "    global_step=epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_configuration/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_configuration/v3.py'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# model\n",
    "model = nn.Sequential(nn.Linear(1,1)).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# MSE loss\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# training step\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# val step\n",
    "val_step = make_val_step(model, loss_fn)\n",
    "\n",
    "# Creates a Summary Writer to inferface with TensorBoard\n",
    "writer = SummaryWriter('../runs/simple_linear_regression')\n",
    "x_dummy, y_dummy = next(iter(train_loader))\n",
    "x_dummy = x_dummy.to(device)\n",
    "y_dummy = y_dummy.to(device)\n",
    "writer.add_graph(model, x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_configuration/v3.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v5.py'\n",
    "\n",
    "# Defines epochs\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # training\n",
    "    loss = mini_batch(device=device,\n",
    "                      data_loader=train_loader,\n",
    "                      step=train_step)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # validation - no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device=device,\n",
    "                              data_loader=val_loader,\n",
    "                              step=val_step)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Records both losses for each epoch under tag 'loss'\n",
    "    writer.add_scalars(main_tag='loss',\n",
    "                        tag_scalar_dict={'train': loss,\n",
    "                                         'val': val_loss},\n",
    "                        global_step=epoch)\n",
    "\n",
    "# close writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_training/v5.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.9330]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.0949], device='cuda:0'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model State\n",
    "- model.state_dict() # Returns a dictionary containing a whole state of the module.\n",
    "- optimizer.state_dict() # Returns a dictionary containing a whole state of the optimizer.\n",
    "- losses\n",
    "- epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': n_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': losses,\n",
    "    'val_loss': val_losses,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run -i '../data_preparation/v2.py'\n",
    "%run -i '../model_configuration/v3.py'\n",
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LOAD THE DICTIONARY BACK USING torch.load()\n",
    "- LOAD MODEL AND OPTIMIZER STATE DICTIONARIES BACK USING THE load_state_dict() METHOD\n",
    "- LOAD EVERYTHING ELSE INTO THEIR CORRESPONDING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Load the last epoch\n",
    "saved_epoch = checkpoint['epoch']\n",
    "saved_losses = checkpoint['loss']\n",
    "saved_val_losses = checkpoint['val_loss']\n",
    "\n",
    "# REMEMBER TO SET THE MODEL TO TRAIN FOR RESUMING TRAINING\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.9330]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.0949], device='cuda:0'))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_training/v5.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[1.0419]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.2501], device='cuda:0'))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying/Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../model_configuration/v3.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.9330]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.0949], device='cuda:0'))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2815],\n",
       "        [1.4121],\n",
       "        [1.6267]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "new_inputs = torch.tensor([[0.20], [0.34], [0.57]])\n",
    "\n",
    "model.eval()\n",
    "model(new_inputs.to(device=device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NOTE: ALWAY SET MODEL MODE:**\n",
    "    - **checkpointing: model.train()**\n",
    "    - **deploying/making predictions: model.eval()**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Preparation V2\n",
    "- Model Configuration V3\n",
    "- Model Training V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../data_preparation/v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../data_preparation/v2.py'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE splitting into train and test\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# Perform split\n",
    "ratio = 0.8\n",
    "n_total = len(train_dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "train_dataset, val_dataset = random_split(train_dataset, [n_train, n_val])\n",
    "\n",
    "# Builds data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_configuration/v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_configuration/v3.py'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# train step\n",
    "train_step = make_train_step(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# val step\n",
    "val_step = make_val_step(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "\n",
    "# Create SummaryWriter to interface with TensorBoard\n",
    "writer = SummaryWriter('../runs/simple_linear_regression')\n",
    "\n",
    "# Fetches a sungle mini-batch of data so we can add graph\n",
    "x, y = next(iter(train_loader))\n",
    "writer.add_graph(model, x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v5.py'\n",
    "\n",
    "# defines number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training \n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # VALIDATION - no gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Records both losses for each epoch under tag 'loss'\n",
    "    writer.add_scalars(\n",
    "                        main_tag='loss', \n",
    "                        tag_scalar_dict = {'train': loss, 'val': val_loss}, \n",
    "                        global_step=epoch)\n",
    "\n",
    "# close writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../data_preparation/v2.py'\n",
    "%run -i '../model_configuration/v3.py'\n",
    "%run -i '../model_training/v5.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.9330]], device='cuda:0')),\n",
       "             ('0.bias', tensor([1.0949], device='cuda:0'))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "\n",
    "- writing a higher-order function that builds functions to perform training steps.\n",
    "- understanding PyTorch's Dataset and DataLoader classes, implementing a custom dataset, and using a DataLoader with a custom dataset.\n",
    "- using PyTorch's DataLoader class to generate mini-batches for training a neural network.\n",
    "- writing higher-order function that builds functions to perform validation steps.\n",
    "- realizeing the importance of including model.eval() and model.train() in the appropriate places when training and validating a model.\n",
    "- remember the purpose of no_grad() and using it to prevent any kind of gradient computation during validation.\n",
    "- using SummaryWriter to interface with TensorBaord for logging.\n",
    "- adding a graph representation of the model to TensorBoard.\n",
    "- using TensorBoard to plot the loss curves for training and validation.\n",
    "- saving/checkpointing a model during training, resuming training from a checkpoint, and loading a model for inference or deployment.\n",
    "- realizeing the importance of setting the mode of the model: train() or eval() for checkpointing or deploying for prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
