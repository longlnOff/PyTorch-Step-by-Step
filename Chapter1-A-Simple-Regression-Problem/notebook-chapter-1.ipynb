{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce MX130\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "device = 'cuda' if torch.cuda.is_available()  else 'cpu'\n",
    "n_cudas = torch.cuda.device_count()\n",
    "for i in range(n_cudas):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400, device='cuda:0')\n",
      "tensor([1, 2, 3], device='cuda:0')\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], device='cuda:0')\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[7, 8, 9],\n",
      "         [0, 1, 2]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14, device=device)\n",
    "vector = torch.tensor([1, 2, 3], device=device)\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]], device=device)\n",
    "tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [0, 1, 2]]], device=device)\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3]), torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.size(), tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5, 6]], device='cuda:0')\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Use view() method to reshape the tensor\n",
    "# Beware: the view() method only returns a tensor with the dsired shape\n",
    "# that shares the same underlying data with original tensor - it DOES NOT create a new tensor!\n",
    "same_matrix = matrix.view(1, -1)\n",
    "print(same_matrix)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100,   2,   3,   4,   5,   6]], device='cuda:0')\n",
      "tensor([[100,   2,   3],\n",
      "        [  4,   5,   6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "same_matrix[0, 0] = 100\n",
    "print(same_matrix)\n",
    "print(matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100,   2,   3,   4,   5,   6]], device='cuda:0')\n",
      "tensor([[1234,    2,    3,    4,    5,    6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# use .clone().detach() to copy a tensor instead of view()\n",
    "other_tensor = same_matrix.clone().detach()\n",
    "other_tensor[0,0] = 1234\n",
    "print(same_matrix)\n",
    "print(other_tensor)\n",
    "# search detach() and clone() in pytorch doc for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1234,    2,    3,    4,    5,    6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if tensor in GPU, we must conver to cpu first, then convert to numpy\n",
    "print(other_tensor.device)\n",
    "other_tensor.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Tensor versus Trainable Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normal Tensor: Doesn't require gradient computation.\n",
    "- Trainable Tensor (Parameter/weight): Requires gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3048], device='cuda:0', requires_grad=True) tensor([-1.2870], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Standard method to create Trainable Tensor\n",
    "# We must specify the requires_grad=True to track computation\n",
    "# and specify the device to be the GPU\n",
    "# this method also accelerates the computation (see 7 PyTorch Tips Github)\n",
    "torch.manual_seed(7)\n",
    "b = torch.randn(1, requires_grad=True, device=device)\n",
    "w = torch.randn(1, requires_grad=True, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd is PyTorch's *automatic differentiation package* that automatically calculates derivatives, chain rule or anything like it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward() method compute all gradients for all (requires_grad=True) tensors involved in the computation of a given variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor([11, 22, 33, 44, 53, 66, 77, 87, 95], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd in action\n",
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train\n",
    "\n",
    "# Step 2 - Computes the loss\n",
    "error = (yhat - y_train)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - Computes the gradients for every parameter with requires_grad=True\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(b.requires_grad, w.requires_grad, loss.requires_grad, error.requires_grad)\n",
    "print(x_train.requires_grad, y_train.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use grad attribute to check actual values of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12218/1389219517.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  b.grad, w.grad, loss.grad, error.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-121.9243], device='cuda:0'),\n",
       " tensor([-769.2258], device='cuda:0'),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad, w.grad, loss.grad, error.grad\n",
    "# grad value of loss and error is None because they are not leaf node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NOTE**: PyTorch default is accumulating gradients. We need to clear them out before each instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every time we use the gradients to update the parameters, we need to zero the gradients afterwards.\n",
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9556], device='cuda:0', requires_grad=True) tensor([0.4135], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is 'eta'\n",
    "lr = 0.0000001\n",
    "\n",
    "# Step 0: Initializes parameters 'b' and 'w' randomly\n",
    "torch.manual_seed(7)\n",
    "b = torch.rand(1, requires_grad=True, device=device, dtype=torch.float32)\n",
    "w = torch.rand(1, requires_grad=True, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Defines nubmer of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    error = y_train - yhat\n",
    "    # MSE\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both 'b' and 'w'\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "\n",
    "    # Zero gradients\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "print(b,w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"283pt\"\n",
       " viewBox=\"0.00 0.00 222.00 283.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-279 218,-279 218,4 -4,4\"/>\n",
       "<!-- 139854654636256 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139854654636256</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (9)</text>\n",
       "</g>\n",
       "<!-- 139854639741296 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139854639741296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139854639741296&#45;&gt;139854654636256 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139854639741296&#45;&gt;139854654636256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n",
       "</g>\n",
       "<!-- 139854639741776 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139854639741776</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139854639741776&#45;&gt;139854639741296 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139854639741776&#45;&gt;139854639741296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n",
       "</g>\n",
       "<!-- 139854659559312 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139854659559312</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-208 23.5,-208 23.5,-177 77.5,-177 77.5,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139854659559312&#45;&gt;139854639741776 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139854659559312&#45;&gt;139854639741776</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.92C50.5,-169.22 50.5,-159.69 50.5,-151.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.25 50.5,-141.25 47,-151.25 54,-151.25\"/>\n",
       "</g>\n",
       "<!-- 139854639742544 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139854639742544</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139854639742544&#45;&gt;139854639741296 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139854639742544&#45;&gt;139854639741296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n",
       "</g>\n",
       "<!-- 139854639742400 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139854639742400</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-202 113,-202 113,-183 214,-183 214,-202\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-190\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139854639742400&#45;&gt;139854639742544 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139854639742400&#45;&gt;139854639742544</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.79C163.5,-174.6 163.5,-162.06 163.5,-151.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.24 163.5,-141.24 160,-151.24 167,-151.24\"/>\n",
       "</g>\n",
       "<!-- 139854653255248 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139854653255248</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-275 136.5,-275 136.5,-244 190.5,-244 190.5,-275\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139854653255248&#45;&gt;139854639742400 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139854653255248&#45;&gt;139854639742400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-243.75C163.5,-234.39 163.5,-222.19 163.5,-212.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-212.02 163.5,-202.02 160,-212.02 167,-212.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f32721f3160>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NOTE**: No gradients, No graph!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use optimizer to update all parameters of model.\n",
    "- Remember, the choice of mini-batch size and optimizer influenced the path of gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step / zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an SGD optimizer to update the parameters\n",
    "optimizer = torch.optim.SGD([b, w], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9546], device='cuda:0', requires_grad=True) tensor([0.4070], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is 'eta'\n",
    "lr = 0.001\n",
    "\n",
    "# Step 0: Initializes parameters 'b' and 'w' randomly\n",
    "torch.manual_seed(7)\n",
    "b = torch.rand(1, requires_grad=True, device=device, dtype=torch.float32)\n",
    "w = torch.rand(1, requires_grad=True, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Defines nubmer of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    error = y_train - yhat\n",
    "    # MSE\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - Computes gradients for both 'b' and 'w'\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    # OLD METHOD\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    # NEW METHOD\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Zero gradients\n",
    "    # OLD METHOD\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    # NEW METHOD\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b,w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NOTE**: You can also specify a reduction method to be apllied, that is how do you want to aggregate the errors for individual points (reduction='mean') or simply sum them up (reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4715], device='cuda:0', requires_grad=True) tensor([10.4031], device='cuda:0', requires_grad=True)\n",
      "loss.item():  1.6535767316818237\n",
      "loss.tolist():  1.6535767316818237\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is 'eta'\n",
    "lr = 1e-5\n",
    "\n",
    "# Step 0: Initializes parameters 'b' and 'w' randomly\n",
    "torch.manual_seed(7)\n",
    "b = torch.rand(1, requires_grad=True, device=device, dtype=torch.float32)\n",
    "w = torch.rand(1, requires_grad=True, device=device, dtype=torch.float32)\n",
    "optimizer = torch.optim.SGD([b, w], lr=lr)\n",
    "\n",
    "\n",
    "# Defines nubmer of epochs\n",
    "n_epochs = 10000\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    # OLD METHOD\n",
    "    # error = y_train - yhat\n",
    "    # # MSE\n",
    "    # loss = (error ** 2).mean()\n",
    "    # NEW METHOD\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    # Step 3 - Computes gradients for both 'b' and 'w'\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b,w)\n",
    "\n",
    "# if we want to convert loss to numpy\n",
    "print('loss.item(): ', loss.item())\n",
    "# or\n",
    "print('loss.tolist(): ', loss.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch, a model is represented by a regular Python class that inherits from the Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # To make 'b' and 'w' real parameters of the model,\n",
    "        # we need to wrap them with nn.Parameter\n",
    "        self.b = torch.nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float32))\n",
    "        self.w = torch.nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute outputs / predictions values\n",
    "        return self.b + self.w * x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In that example, we use torch.nn.Parameter() to create parameters for model. This tell PyTorch that these tensors are attributes of the class, should be considered parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.5349], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1988], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "dummy_model = ManualLinearRegression()\n",
    "list(dummy_model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.5349])), ('w', tensor([0.1988]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state_dict() method only returns **leanable parameters** of model, as it purpose is to keep track of parameters that are going to be updated by the **optimizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}},\n",
       " 'param_groups': [{'lr': 1e-05,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False,\n",
       "   'params': [0, 1]}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer has state_dict too, which contain hyper-parameter of optimizer\n",
    "optimizer.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data and Model need to be in the same device if we want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ManualLinearRegression().to(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([2.1100], device='cuda:0')), ('w', tensor([10.4632], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is 'eta'\n",
    "lr = 1e-3\n",
    "\n",
    "# Step 0: Initializes parameters 'b' and 'w' randomly\n",
    "torch.manual_seed(7)\n",
    "# Create model and send it to device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Defines optimizer \n",
    "# Retrive parameters directly from the model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines MSE loss\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines nubmer of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()       # Set model in training mode\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = model(x_train)\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "\n",
    "    # Step 3 - Computes gradients for both 'b' and 'w'\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "model_linear = MyLinearRegression().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.0698]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.6024], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_linear.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[0.3184]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.3138], device='cuda:0'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sequential = torch.nn.Sequential(torch.nn.Linear(1,1)).to(device=device)\n",
    "model_sequential.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer 1): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (layer 2): Linear(in_features=1, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module('layer 1', torch.nn.Linear(1,1))\n",
    "model.add_module('layer 2', torch.nn.Linear(1,5))\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer n1): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (layer n2): Linear(in_features=1, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model_sequential = torch.nn.Sequential(OrderedDict([\n",
    "                                        ('layer n1', torch.nn.Linear(1,1)),\n",
    "                                        ('layer n2', torch.nn.Linear(1,5))\n",
    "                                                    ]\n",
    "                                        )).to(device=device)\n",
    "model_sequential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../data_preparation/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../data_preparation/v0.py'\n",
    "\n",
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (0.1 * np.random.rand(N, 1))\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*0.8)]\n",
    "# Uses remaining indices for validation\n",
    "val_idx = idx[int(N*0.8):]\n",
    "\n",
    "# Generate train and validatrion sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val = x[val_idx], y[val_idx]\n",
    "\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device=device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../model_configuration/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_configuration/v0.py'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create model and send it to device\n",
    "model = torch.nn.Sequential(torch.nn.Linear(1,1)).to(device)\n",
    "\n",
    "# Define SGD optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../model_training/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../model_training/v0.py'\n",
    "\n",
    "# Defines n_epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Set model to TRAIN mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1 - Forward pass\n",
    "    y_pred = model(x_train_tensor)\n",
    "\n",
    "    # Step 2 - Compute Loss\n",
    "    loss = loss_fn(y_pred, y_train_tensor)\n",
    "\n",
    "    # Step 3 - Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Update parameters using gradients and the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.2059]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([1.4151], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "%run -i '../data_preparation/v0.py'\n",
    "%run -i '../model_configuration/v0.py'\n",
    "%run -i '../model_training/v0.py'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating tensors in PyTorch, sending them to a device and making parameters out of them.\n",
    "- Understanding PyTorch's main feature, autograd to perform automatic differentiation uisng it associated properties and methods like: backward, grad, zero_, no_grad.\n",
    "- Visualizing the Dynamic Computational Graphs (DCG) of PyTorch associated with a sequence of operations.\n",
    "- Creating an optimizer to perform gradient descent and update the parameters of the model, using its step and zero_grad methods.\n",
    "- Creating a loss function uisngPyTorch's higher-order function.\n",
    "- Understanding PyTorch's Module class and creating your own models, implementing __init__ and forward methods and making use of its built-in parameters and state_dict methods.\n",
    "- Realizing the importance of including model.train() in the training loop.\n",
    "Implementing nested and sequential models using PyTorch's layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
